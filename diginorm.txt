Apply digital normalization to the paired-end reads:
====================================================

Normalize everything to a coverage of 20, starting with th (more valuable) PE reads; keep using '-p'.

normalize-by-median.py -p -k 20 -C 20 -N 4 -x 3e8 --savetable normC20k20.kh *.pe.qc.fq.gz

and then to the (less values but may be still useful) single-end reads:
normalize-by-median.py -C 20 --loadtable normC20k20.kh --savetable normC20k20.kh *.se.qc.fq.gz

Note the ‘-p’ in the first normalize-by-median command – when run on PE data, that ensures that no paired ends are orphaned. However, it will complain on single-ended data, so you have to give the data to it separately.

Also note the ‘-N’ and ‘-x’ parameters. These specify how much memory diginorm should use. The product of these should be less than the memory size of the machine you selected. The maximum needed for any transcriptome should be in the ~60 GB range, e.g. -N 4 -x 15e9; for only a few hundred million reads, 16 GB should be plenty. 

This produces a set of .keep' files, as well as a normC20k20.kh database file. 

Trim off likely erroneous k-mers: 
=================================

Use ‘filter-abund’ to trim off any k-mers that are abundance-1 in high-coverage reads. The -V option is used to make this work better for variable coverage data sets:

/usr/local/share/khmer/scripts/filter-abund.py -V normC20k20.kh *.keep

This produces .abundfilt files containing the trimmed sequences.

The process of error trimming could have orphaned reads, so split the PE file into still-interleaved and non-interleaved reads:

Rename files:
=============

First, let’s break out the orphaned and still-paired reads:

for i in *.pe.qc.fq.gz.keep.abundfilt
do
   /usr/local/share/khmer/scripts/extract-paired-reads.py $i
done

This leaves you with PE files (.pe.qc.fq.gz.keep.abundfilt.pe) and two sets of SE files (.se.qc.fq.gz.keep.abundfilt and .pe.qc.fq.gz.keep.abundfilt.se). (Yes, the naming scheme does make sense. Trust me.)

Normalize down to C=5:
======================

Now that we’ve eliminated many more erroneous k-mers, let’s ditch some more high-coverage data. First, normalize the paired-end reads:

/usr/local/share/khmer/scripts/normalize-by-median.py -C 5 -k 20 -N 4 -x 5e8 --savetable normC5k20.kh -p *.pe.qc.fq.gz.keep.abundfilt.pe

and then do the remaining single-ended reads:

/usr/local/share/khmer/scripts/normalize-by-median.py -C 5 --savetable normC5k20.kh --loadtable normC5k20.kh *.pe.qc.fq.gz.keep.abundfilt.se *.se.qc.fq.gz.keep.abundfilt

Compress and combine the files:
===============================

Now let’s tidy things up. Here are the paired files (kak = keep/abundfilt/keep):

gzip -9c SRR606249.pe.qc.fq.gz.keep.abundfilt.pe.keep > SRR606249.pe.kak.qc.fq.gz

and the single-ended files:

gzip -9c SRR606249.pe.qc.fq.gz.keep.abundfilt.se.keep SRR606249.se.qc.fq.gz.keep.abundfilt.keep > SRR606249.se.kak.qc.fq.gz
